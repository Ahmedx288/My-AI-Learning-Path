# Optimization Algorithms

**Learning Objectives**
* Apply optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam.
* Use random minibatches to accelerate convergence and improve optimization.
* Describe the benefits of learning rate decay and apply it to your optimization.

**This week contains:** *1 Quizz & 1 Programming Exercise*

----

**Module 1: Optimization Algorithms**
|Lecture|Duration|
|--|--|
|Mini-batch gradient descent|(11 min)|
|Understanding mini-batch gradient descent|(11 min)|
|Exponentially weighted averages|(5 min)|
|Understanding exponentially weighted averages|(9 min)|
|Bias correction in exponentially weighted averages|(4 min)|
|Gradient descent with momentum|(9 min)|
|RMSprop|(7 min)|
|Adam optimization algorithm|(7 min)|
|Learning rate decay|(6 min)|
|The problem of local optima|(5 min)|


