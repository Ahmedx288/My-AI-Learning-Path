# Practical aspects of Deep Learning

**Learning Objectives**
* Give examples of how different types of initializations can lead to different results.
* Examine the importance of initialization in complex neural networks.
* Explain the difference between train/dev/test sets.
* Diagnose the bias and variance issues in your model.
* Assess the right time and place for using regularization methods such as dropout or L2 regularization.
* Explain Vanishing and Exploding gradients and how to deal with them.
* Use gradient checking to verify the accuracy of your backpropagation implementation.

**This week contains:** *1 Quizz & 3 Programming Exercises*

----

**Module 1: Setting up your Machine Learning Application**
|Lecture|Duration|
|--|--|
|Train / Dev / Test sets|(12 min)|
|Bias / Variance|(8 min)|
|Basic Recipe for Machine Learning|(6 min)|

**Module 2: Regularizing your neural network**
|Lecture|Duration|
|--|--|
|Regularization|(9 min)|
|Why regularization reduces overfitting?|(7 min)|
|Dropout Regularization|(9 min)|
|Understanding Dropout|(7 min)|
|Other regularization methods|(8 min)|

**Module 3: Setting up your optimization problem**
|Lecture|Duration|
|--|--|
|Normalizing inputs|(5 min)|
|Vanishing / Exploding gradients|(6 min)|
|Weight Initialization for Deep Networks|(6 min)|
|Numerical approximation of gradients|(6 min)|
|Gradient checking|(6 min)|
|Gradient Checking Implementation Notes|(5 min)|


