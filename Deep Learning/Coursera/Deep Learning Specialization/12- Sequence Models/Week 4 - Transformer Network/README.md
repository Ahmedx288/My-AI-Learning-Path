# Transformer Network

**Learning Objectives**
* Create positional encodings to capture sequential relationships in data.
* Calculate scaled dot-product self-attention with word embeddings.
* Implement masked multi-head attention.
* Build and train a Transformer model.
* Fine-tune a pre-trained transformer model for Named Entity Recognition.
* Fine-tune a pre-trained transformer model for Question Answering.
* Implement a QA model in TensorFlow and PyTorch.
* Fine-tune a pre-trained transformer model to a custom dataset.
* Perform extractive Question Answering.


**This week contains:** *1 Quiz & 1 Programming Exercise & 2 Ungraded Labs*

----

**Module 1: Transformers**
|Lecture|Duration|
|--|--|
|Transformer Network Intuition|(5 min)|
|Self-Attention|(11 min)|
|Multi-Head Attention|(8 min)|
|Transformer Network|(13 min)|

**Module 2: Conclusion**
|Lecture|Duration|
|--|--|
|Conclusion and Thank You!|(2 min)|