# Natural Language Processing & Word Embeddings

Natural language processing with deep learning is a powerful combination.

Using word vector representations and embedding layers, train recurrent neural networks with outstanding performance across a wide variety of applications, including sentiment analysis, named entity recognition and neural machine translation.

**Learning Objectives**
* Explain how word embeddings capture relationships between words.
* Load pre-trained word vectors.
* Measure similarity between word vectors using cosine similarity.
* Use word embeddings to solve word analogy problems such as Man is to Woman as King is to ______.
* Reduce bias in word embeddings.
* Create an embedding layer in Keras with pre-trained word vectors.
* Describe how negative sampling learns word vectors more efficiently than other methods.
* Explain the advantages and disadvantages of the GloVe algorithm.
* Build a sentiment classifier using word embeddings.
* Build and train a more sophisticated classifier using an LSTM.

**This week contains:** *1 Quiz & 2 Programming Exercises*

----

**Module 1: Introduction to Word Embeddings**
|Lecture|Duration|
|--|--|
|Word Representation|(10 min)|
|Using Word Embeddings|(9 min)|
|Properties of Word Embeddings|(11 min)|
|Embedding Matrix|(5 min)|

**Module 2: Learning Word Embeddings: Word2vec & GloVe**
|Lecture|Duration|
|--|--|
|Learning Word Embeddings|(10 min)|
|Word2Vec|(12 min)|
|Negative Sampling|(11 min)|
|GloVe Word Vectors|(11 min)|

**Module 3: Applications Using Word Embeddings**
|Lecture|Duration|
|--|--|
|Sentiment Classification|(7 min)|
|Debiasing Word Embeddings|(11 min)|