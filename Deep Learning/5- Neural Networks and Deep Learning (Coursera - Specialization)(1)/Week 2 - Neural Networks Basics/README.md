# Neural Networks Basics
Learn to set up a machine learning problem with a neural network mindset.
Learn to use vectorization to speed up your models.

**Learning Objectives:**
* Build a logistic regression model structured as a shallow neural network.
* Build the general architecture of a learning algorithm, including parameter initialization, cost function and gradient calculation, and optimization implemetation (gradient descent).
* Implement computationally efficient and highly vectorized versions of models.
* Compute derivatives for logistic regression, using a backpropagation mindset.
* Use Numpy functions and Numpy matrix/vector operations.
* Work with iPython Notebooks.
* Implement vectorization across multiple training examples.

**This week contains:** *1 Quizz & 2 Programming Exercises*

----

**Module 1: Logistic Regression as a Neural Network**
|Lecture|Duration|
|--|--|
|Binary Classification|(8 min)|
|Logistic Regression|(5 min)|
|Logistic Regression Cost Function|(8 min)|
|Gradient Descent|(11 min)|
|Derivatives|(7 min)|
|More Derivative Examples|(10 min)|
|Computation graph|(3 min)|
|Derivatives with a Computation Graph|(14 min)|
|Logistic Regression Gradient Descent|(6 min)|
|Gradient Descent on m Examples|(8 min)|
|Derivation of DL/dz (optional reading)|(10 min)|

**Module 2: Python and Vectorization**
|Lecture|Duration|
|--|--|
|Vectorization|(8 min)|
|More Vectorization Examples|(6 min)|
|Vectorizing Logistic Regression|(7 min)|
|Vectorizing Logistic Regression's Gradient Output|(9 min)|
|Broadcasting in Python|(11 min)|
|A note on python/numpy vectors|(6 min)|
|Quick tour of Jupyter/iPython Notebooks|(3 min)|
|Explanation of logistic regression cost function (optional)|(7 min)|